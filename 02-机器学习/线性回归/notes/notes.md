# 线性回归 — 学习笔记

> 📌 线性回归是机器学习中最基础、最重要的算法之一，理解它是迈向更复杂模型的第一步。

---

## 一、算法核心思想

### 1. 基本概念

线性回归（Linear Regression）是一种**监督学习**算法，用于预测**连续型**目标变量。

> ⭐ **核心思想**：假设输入特征 x 与输出 y 之间存在**线性关系**，通过学习一组最优参数，使模型的预测值尽可能接近真实值。

### 2. 模型表示

对于单个特征（一元线性回归）：
```
y_pred = w * x + b
```

对于多个特征（多元线性回归）：
```
y_pred = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
```

其中：
- y_pred：预测值（目标变量）
- x₁...xₙ：特征变量
- w₁...wₙ：**权重系数**（模型要学习的参数）
- b：**偏置项**（截距）

> ⭐ **重点**：线性回归的"线性"指的是参数 w 和 b 的线性组合，并不意味着只能拟合直线——通过**特征变换**（如 x², √x），线性回归也能拟合曲线关系。

---

## 二、重要数学概念

### 1. 损失函数（Loss Function）

> ⭐ **损失函数衡量的是模型预测值与真实值之间的偏差程度，是模型优化的目标。**

线性回归最常用的损失函数是**均方误差**（Mean Squared Error, MSE）：

```
均方误差（MSE） = (1/m) * Σ(y_pred - y_true)²
```

其中：
- m：样本数量
- y_pred：模型预测值
- y_true：真实值
- 目标：最小化 MSE，找到最优 w 和 b

> 📌 **为什么用"平方"？**
> - 平方保证误差为**非负数**，正负误差不会相互抵消
> - 平方函数是**光滑可导**的，便于使用梯度下降优化
> - MSE 对应了**高斯噪声假设**下的最大似然估计

#### 其他常见损失函数对比

| 损失函数 | 公式 | 特点 |
|---------|------|------|
| **MSE（均方误差）** | (1/m) * Σ(y_pred - y_true)² | ⭐ 最常用，对大误差敏感 |
| MAE（平均绝对误差） | (1/m) * Σ|y_pred - y_true| | 对异常值更鲁棒，但不可导 |
| Huber Loss | MSE 与 MAE 的结合 | 综合两者优点 |

---

### 2. 梯度下降（Gradient Descent）

> ⭐ **梯度下降是一种通用的优化算法，用于迭代地寻找使损失函数最小化的参数。**

#### 核心直觉

想象你站在山上，蒙着眼睛想走到最低点：
- **梯度**告诉你"最陡的上坡方向"
- 沿着梯度的**反方向**走，就是下山最快的方向
- **学习率**决定了每一步迈多大

#### 参数更新公式

```
w_new = w_old - α * ∂L/∂w
b_new = b_old - α * ∂L/∂b
```

- α：学习率（控制更新步长）
- ∂L/∂w：损失函数对权重 w 的偏导数（梯度）

**梯度计算推导**（单特征情况）：

```
L(w,b) = (1/m)Σ(wxᵢ + b - yᵢ)² --损失函数(具体来说是均方误差MSE)
∂L/∂w = (2/m)Σ(wxᵢ + b - yᵢ) * xᵢ
∂L/∂b = (2/m)Σ(wxᵢ + b - yᵢ)
```

**梯度计算推导**（多特征情况）：

当有 n 个特征时，模型为 `y_pred = w₁x₁ + w₂x₂ + ... + wₙxₙ + b`

```
L(w₁,...,wₙ,b) = (1/m)Σ(w₁x₁ᵢ + w₂x₂ᵢ + ... + wₙxₙᵢ + b - yᵢ)² --同一样本的多个特征值

对每个权重 wⱼ 求偏导：
∂L/∂wⱼ = (2/m)Σ(w₁x₁ᵢ + w₂x₂ᵢ + ... + wₙxₙᵢ + b - yᵢ) * xⱼᵢ

对偏置 b 求偏导（与单特征相同）：
∂L/∂b = (2/m)Σ(w₁x₁ᵢ + w₂x₂ᵢ + ... + wₙxₙᵢ + b - yᵢ)
```

> 📌 **单特征 vs 多特征的区别**：
> - 单特征：只需更新 1 个权重 w
> - 多特征：需要**同时更新** n 个权重 w₁, w₂, ..., wₙ，每个 wⱼ 的梯度乘的是对应的特征 xⱼ
> - 偏置 b 的更新方式不变

> ⭐ **学习率 α 的选择至关重要**：
> - **太大** → 可能跳过最优点，导致震荡甚至发散 ❌
> - **太小** → 收敛速度极慢，训练耗时过长 ❌
> - **合适** → 稳定收敛到最优解 ✅

#### 梯度下降的三种变体

| 变体 | 每次使用的样本数 | 特点 |
|------|----------------|------|
| **批量梯度下降（BGD）** | 全部样本 m | 稳定但慢 |
| **随机梯度下降（SGD）** | 1 个样本 | 快但震荡大 |
| **小批量梯度下降（Mini-batch）** | 一小批样本 | ⭐ 实践中最常用，兼顾速度与稳定性 |

---

### 3. 损失函数与梯度下降的联系

> ⭐ **核心关系：损失函数定义了"优化目标"，梯度下降提供了"优化方法"。**

它们的协作流程如下：

```
初始化参数 w, b
      │
      ▼
┌─────────────────┐
│  前向传播：计算 ŷ  │
└────────┬────────┘
         │
         ▼
┌─────────────────────┐
│  计算损失函数 L(w,b)  │  ← 损失函数：衡量差距有多大
└────────┬────────────┘
         │
         ▼
┌─────────────────────────┐
│  计算梯度 ∂L/∂w, ∂L/∂b  │  ← 梯度：指明优化方向
└────────┬────────────────┘
         │
         ▼
┌─────────────────────────┐
│  更新参数 w, b（梯度下降）│  ← 梯度下降：执行优化
└────────┬────────────────┘
         │
         ▼
   损失足够小？──否──→ 回到前向传播
         │
        是
         │
         ▼
      训练完成
```

> 📌 **关键理解**：
> - 损失函数构建了一个关于参数 (w, b) 的**误差曲面**
> - 梯度下降在这个曲面上寻找**最低点**（全局最小值）
> - 对于线性回归的 MSE，误差曲面是一个**凸函数**（碗状），因此梯度下降**一定能找到全局最优解**

---

### 4. 正规方程（解析解）

除了梯度下降，线性回归还可以通过**正规方程**直接求解：

```
w* = (Xᵀ X)⁻¹ Xᵀ y
```

| 方法 | 优点 | 缺点 |
|------|------|------|
| 正规方程 | 一步到位，无需迭代 | 特征维度高时计算量大 O(n³) |
| 梯度下降 | ⭐ 适用于大规模数据 | 需要调学习率，需多次迭代 |

> ⭐ **经验法则**：特征数 n < 10000 时可用正规方程；n 更大时优先用梯度下降。

---

## 三、应用场景与实例

### 典型应用场景

| 场景 | 输入特征 x | 预测目标 y |
|------|-------------|-------------|
| 🏠 **房价预测** | 面积、房间数、地段 | 房屋价格 |
| 📈 **销售预测** | 广告投入、季节、历史销量 | 未来销售额 |
| 🌡️ **气温预测** | 日期、湿度、风速 | 温度 |
| 💰 **薪资估算** | 工作年限、学历、技能 | 年薪 |
| 📊 **股票趋势** | 历史价格、交易量 | 价格走势（短期） |

### 实例：房价预测

**问题**：已知若干房屋的面积与对应价格，预测新房屋的价格。

- 特征 x：房屋面积（平方米）
- 目标 y：价格（万元）
- 模型：y_pred = w * x + b

**学习过程**：
1. 收集数据：(60m², 150万), (80m², 200万), (120m², 320万), ...
2. 用 MSE 作为损失函数
3. 通过梯度下降迭代优化，学得 w ≈ 2.8，b ≈ -18
4. 预测：面积 100m² 的房屋价格 ≈ 2.8 × 100 - 18 = 262 万元

---

## 四、重点总结

> 🔑 **必须掌握的核心要点**：

1. **线性回归**假设特征与目标之间是线性关系，通过学习参数 w, b 来拟合数据
2. **损失函数（MSE）** 量化了预测误差，是优化的目标 → *让损失越小越好*
3. **梯度下降**通过计算梯度、沿反方向更新参数来最小化损失 → *迭代逼近最优解*
4. **学习率**是最关键的超参数，直接影响训练效果
5. MSE 损失函数是**凸函数**，保证梯度下降能找到**全局最优**
6. 线性回归是理解更复杂模型（逻辑回归、神经网络）的**基石**

---

*📅 笔记创建日期：2026-02-24*
